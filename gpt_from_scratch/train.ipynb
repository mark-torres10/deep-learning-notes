{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT from scratch\n",
    "\n",
    "Follows [this](https://www.youtube.com/watch?v=kCc8FmEb1nY) video tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !curl -O https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of text: {len(text)}\")\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "vocab = {c: i for i, c in enumerate(chars)}\n",
    "print(len(vocab)) # 65 unique chars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to now be able to tokenize the text somehow. Here, we're building a character-level model, so we're just translating individual characters into integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {c: i for i, c in enumerate(chars)}\n",
    "itos = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "# take a string, return a list of integers\n",
    "encode = lambda x: [stoi[c] for c in x]\n",
    "# take a list of integers, return a string\n",
    "decode = lambda x: ''.join([itos[i] for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 43, 50, 50, 53, 1, 58, 46, 47, 57, 1, 47, 57, 1, 39, 1, 58, 43, 57, 58, 1, 57, 58, 56, 47, 52, 45]\n",
      "Hello this is a test string\n"
     ]
    }
   ],
   "source": [
    "print(encode(\"Hello this is a test string\"))\n",
    "print(decode(encode(\"Hello this is a test string\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now tokenize the entire text dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's also split out data into train and validation datasets\n",
    "n = int(0.9*len(data))\n",
    "train_data, val_data = data[:n], data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't pass in the entire dataset in one iteration of the transformer, so we do need to set a \"block size\" (AKA context length)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at one block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training using chunks of the context window\n",
    "A transformer makes simultaneous predictions for each of these positions. This makes training much more efficient since we treat each position as a separate prediction task instead of just predicting the next character after the last position, which greatly increases the number of samples used for training.\n",
    "\n",
    "This is why we take `[:block_size+1]` since if we have 8 chars, we need to look 9 chars so that we can always predict the char after a given char.\n",
    "\n",
    "We can actually display what the prediction task is: given a block of text, we predict what token is at each position. The input passed into each prediction task is whatever tokens were in the block before this target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is tensor([18]), the target is: 47\n",
      "When input is tensor([18, 47]), the target is: 56\n",
      "When input is tensor([18, 47, 56]), the target is: 57\n",
      "When input is tensor([18, 47, 56, 57]), the target is: 58\n",
      "When input is tensor([18, 47, 56, 57, 58]), the target is: 1\n",
      "When input is tensor([18, 47, 56, 57, 58,  1]), the target is: 15\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15]), the target is: 47\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47]), the target is: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"When input is {context}, the target is: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in a chunk of 9 characters, we essentially have 8 training examples. This not only gives us more training samples, but it also makes the Transformer used to seeing context sizes from as little as 1 all the way to the context length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching\n",
    "\n",
    "We also need to care about the batching dimension as well, since we'll be feeding the input as batches of tensors. This lets us process more inputs in parallel, especially since GPUs are great at working with batches of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x114a0e470>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of sequences in a batch, processed in parallel.\n",
    "batch_size = 4\n",
    "\n",
    "# maximum context length for prediction\n",
    "block_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    \"\"\"Gets a batch of data for training or validation.\"\"\"\n",
    "    data = (\n",
    "        train_data if split == \"train\" else val_data\n",
    "    )\n",
    "    # we take random ints as our start indices. We make\n",
    "    # sure to take random ints that allow us to take a full\n",
    "    # sequence of `block_size` length\n",
    "    ix = torch.randint(\n",
    "        len(data) - block_size, # which ints to include in pool\n",
    "        (batch_size,) # number of results = batch size.\n",
    "    )\n",
    "\n",
    "    # for each start index, we take a random block of chars.\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb = get_batch(split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at our batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0\n",
      "----------\n",
      "When input is tensor([43]), the target is: 1\n",
      "When input is tensor([43,  1]), the target is: 51\n",
      "When input is tensor([43,  1, 51]), the target is: 39\n",
      "When input is tensor([43,  1, 51, 39]), the target is: 63\n",
      "When input is tensor([43,  1, 51, 39, 63]), the target is: 1\n",
      "When input is tensor([43,  1, 51, 39, 63,  1]), the target is: 40\n",
      "When input is tensor([43,  1, 51, 39, 63,  1, 40]), the target is: 43\n",
      "When input is tensor([43,  1, 51, 39, 63,  1, 40, 43]), the target is: 1\n",
      "----------\n",
      "Batch 1\n",
      "----------\n",
      "When input is tensor([58]), the target is: 46\n",
      "When input is tensor([58, 46]), the target is: 43\n",
      "When input is tensor([58, 46, 43]), the target is: 1\n",
      "When input is tensor([58, 46, 43,  1]), the target is: 43\n",
      "When input is tensor([58, 46, 43,  1, 43]), the target is: 39\n",
      "When input is tensor([58, 46, 43,  1, 43, 39]), the target is: 56\n",
      "When input is tensor([58, 46, 43,  1, 43, 39, 56]), the target is: 57\n",
      "When input is tensor([58, 46, 43,  1, 43, 39, 56, 57]), the target is: 10\n",
      "----------\n",
      "Batch 2\n",
      "----------\n",
      "When input is tensor([39]), the target is: 58\n",
      "When input is tensor([39, 58]), the target is: 47\n",
      "When input is tensor([39, 58, 47]), the target is: 53\n",
      "When input is tensor([39, 58, 47, 53]), the target is: 52\n",
      "When input is tensor([39, 58, 47, 53, 52]), the target is: 12\n",
      "When input is tensor([39, 58, 47, 53, 52, 12]), the target is: 1\n",
      "When input is tensor([39, 58, 47, 53, 52, 12,  1]), the target is: 37\n",
      "When input is tensor([39, 58, 47, 53, 52, 12,  1, 37]), the target is: 53\n",
      "----------\n",
      "Batch 3\n",
      "----------\n",
      "When input is tensor([53]), the target is: 56\n",
      "When input is tensor([53, 56]), the target is: 43\n",
      "When input is tensor([53, 56, 43]), the target is: 1\n",
      "When input is tensor([53, 56, 43,  1]), the target is: 21\n",
      "When input is tensor([53, 56, 43,  1, 21]), the target is: 1\n",
      "When input is tensor([53, 56, 43,  1, 21,  1]), the target is: 41\n",
      "When input is tensor([53, 56, 43,  1, 21,  1, 41]), the target is: 39\n",
      "When input is tensor([53, 56, 43,  1, 21,  1, 41, 39]), the target is: 51\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for b in range(batch_size):\n",
    "    print(f\"Batch {b}\")\n",
    "    print('-' * 10)\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"When input is {context}, the target is: {target}\")\n",
    "    print('-' * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these batch samples becomes a row in the data. What we end up getting is a `(batch_size, block_size)`-shaped tensor for our training data.For batch_size=4 and block_size=8, this leads to a 4x8 tensor.\n",
    "\n",
    "Because we predict the next token for each position in the block, we get 8 training samples out of a block of size=8. This means that in this batch, we have 32 training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting neural network training with the simplest model: bigrams\n",
    "Now we can start using this input for neural network training. We can start with the simplest model: the bigram.\n",
    "\n",
    "#### What is a bigram model?\n",
    "A bigram model is a model that predicts the next word given the previous word in a sentence. It assumes that the probability of the next word is *only affected by the previous word*.\n",
    "\n",
    "For our case, we'll use a bigram character model, where the probability of a given character is based only on the probability of the previous character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x114a0e470>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    \"\"\"Bigram model.\"\"\"\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # read off the logits for the next token\n",
    "        # from a lookup table.\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "    \n",
    "    def forward(self, idx):\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        # get the logits for the next token.\n",
    "        # for a given batch with shape (B, T), we will get an output\n",
    "        # of shape (B, T, C), where B = batch size, T = sequence length\n",
    "        # and C = vocab size.\n",
    "        logits = self.token_embedding_table(idx)\n",
    "        return logits\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"Predict the next token.\"\"\"\n",
    "        # get the logits\n",
    "        logits = self(x)\n",
    "        # get argmax\n",
    "        return logits.argmax(dim=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this would look so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.token_embedding_table.weight.shape=torch.Size([65, 65])\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel(vocab_size=vocab_size)\n",
    "print(f\"{model.token_embedding_table.weight.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a naive prediction. Given a particular index in our vocabulary, let's see what the predicted output is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input index: 5, Prediction index: 38\n",
      "Output logits shape: torch.Size([1, 65])\n",
      "Input: ', Prediction: Z\n"
     ]
    }
   ],
   "source": [
    "test_idx = 5\n",
    "logits = model(torch.tensor([test_idx]))\n",
    "prediction = model.predict(torch.tensor([test_idx]))\n",
    "print(f\"Input index: {test_idx}, Prediction index: {prediction.item()}\")\n",
    "# tensor of shape (1, vocab_size), with each element being the\n",
    "# logit for each token in the vocabulary at the given index.\n",
    "print(f\"Output logits shape: {logits.shape}\")\n",
    "# we take the max index of the logits to get the\n",
    "# predicted token.\n",
    "print(f\"Input: {itos[test_idx]}, Prediction: {itos[prediction.item()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it on a batch of texts. Let's get the accuracy based on a random set of results. When we run `forward`, we get a result of shape `(batch_size, block_size, vocab_size)`. We get a `[vocab_size]`-shaped tensor for all positions in our `[batch_size, block_size]`-shaped batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_logits = model(xb)\n",
    "preds = model.predict(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_logits.shape=torch.Size([4, 8, 65])\n",
      "preds.shape=torch.Size([4, 8])\n",
      "yb.shape=torch.Size([4, 8])\n",
      "Accuracy: 0.03125\n"
     ]
    }
   ],
   "source": [
    "# batch logits shape = (batch_size, block_size, vocab_size)\n",
    "print(f\"{batch_logits.shape=}\")\n",
    "\n",
    "# preds shape = (batch_size, block_size) because we\n",
    "# take the argmax on the -1 dim to get the predicted token.\n",
    "print(f\"{preds.shape=}\")\n",
    "print(f\"{yb.shape=}\")\n",
    "print(f\"Accuracy: {(preds == yb).float().mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model only has an accuracy of 3%. Again, this makes sense given that our model is randomly initialized.\n",
    "\n",
    "Of course this doesn't have any actual basis, since this is based on a randomly initialized lookup table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add a loss function to our model so we can have a way to evaluate \"how good\" the predictions are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    \"\"\"Bigram model.\"\"\"\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # read off the logits for the next token\n",
    "        # from a lookup table.\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        # get the logits for the next token.\n",
    "        # for a given batch with shape (B, T), we will get an output\n",
    "        # of shape (B, T, C), where B = batch size, T = sequence length\n",
    "        # and C = vocab size.\n",
    "        logits = self.token_embedding_table(idx)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            # flatten the logits\n",
    "            logits = logits.view(B*T, C)\n",
    "            # flatten the targets\n",
    "            targets = targets.view(B*T)\n",
    "            # compute the loss\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"Predict the next token.\"\"\"\n",
    "        # get the logits\n",
    "        logits = self(x)\n",
    "        # get argmax\n",
    "        return logits.argmax(dim=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get the predictions and loss. We get the logits for all `[batch_size, block_size]` characters as well as the total loss.\n",
    "\n",
    "We're expecting something like `-ln(1/65) ~ 4.17`, which is what the loss would be if we were accurate `1/65` of the time (which is expected for a random model with vocab size of 65)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits.shape=torch.Size([32, 65])\n",
      "loss=tensor(4.7801, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel(vocab_size=vocab_size)\n",
    "logits, loss = model(xb, yb)\n",
    "print(f\"{logits.shape=}\")\n",
    "print(f\"{loss=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can evaluate the loss, we'd like to also be able to do generation from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    \"\"\"Bigram model.\"\"\"\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # read off the logits for the next token\n",
    "        # from a lookup table.\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        # get the logits for the next token.\n",
    "        # for a given batch with shape (B, T), we will get an output\n",
    "        # of shape (B, T, C), where B = batch size, T = sequence length\n",
    "        # and C = vocab size.\n",
    "        logits = self.token_embedding_table(idx)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            # flatten the logits\n",
    "            logits = logits.view(B*T, C)\n",
    "            # flatten the targets\n",
    "            targets = targets.view(B*T)\n",
    "            # compute the loss\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"Predict the next token.\"\"\"\n",
    "        # get the logits\n",
    "        logits = self(x)\n",
    "        # get argmax\n",
    "        return logits.argmax(dim=-1)\n",
    "\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        \"\"\"Generate new tokens.\n",
    "        \n",
    "        idx is (B,T) shaped tensor with array of indices\n",
    "        that are in the current context.\n",
    "\n",
    "        At each step, we predict the next token and add it to the context.\n",
    "\n",
    "        Algorithm:\n",
    "            - We get the logits for the last token in the context for each sequence\n",
    "            in the batch\n",
    "            - We apply softmax to get probabilities for the next token, for each\n",
    "            sequence in the batch.\n",
    "            - We sample from the distribution to get the next token for each sequence\n",
    "            in the batch.\n",
    "            - We add the new token to the context and repeat.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get predictions\n",
    "            logits, _ = self(idx)\n",
    "            # focus on last time stamp\n",
    "            # becomes (B, C) since we take the logits of the\n",
    "            # last position in each sequence of the batch.\n",
    "            logits = logits[:, -1, :]\n",
    "            # apply softmax to get probabilities.\n",
    "            # (B, C), but now values are probabilities.\n",
    "            # Each row is a probability distribution whose values add\n",
    "            # up to 1.\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from the distribution\n",
    "            # (B, 1). We get the next token for each sample in the batch.\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            # we add the new token to the context and repeat.\n",
    "            # idx originally (B, T), next_token (B, 1), so when we concatenate\n",
    "            # we get (B, T+1)\n",
    "            idx = torch.cat([idx, next_token], dim=1) # (B, T+1)\n",
    "\n",
    "        # shape of idx is (B, T+max_new_tokens) since we've added\n",
    "        # max_new_tokens to the context.\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BigramLanguageModel(vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xb.shape=torch.Size([32, 8])\n",
      "generated_tokens.shape=torch.Size([32, 108])\n"
     ]
    }
   ],
   "source": [
    "max_new_tokens = 100\n",
    "generated_tokens = model.generate(xb, max_new_tokens)\n",
    "# batch_size of 4\n",
    "# 8 initial tokens\n",
    "# => (4, 8)\n",
    "print(f\"{xb.shape=}\")\n",
    "# batch_size of 4\n",
    "# for each sequence, 8 initial tokens + 100 generated tokens.\n",
    "# => (4, 108)\n",
    "print(f\"{generated_tokens.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's decode both of these to see the actual letters. First, let's look at our batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String 0: e may be\n",
      "String 1: the ears\n",
      "String 2: ation? Y\n",
      "String 3: ore I ca\n"
     ]
    }
   ],
   "source": [
    "decoded_batch = []\n",
    "for seq in xb:\n",
    "    output_str = \"\"\n",
    "    for idx in seq:\n",
    "        output_str += itos[idx.item()]\n",
    "    decoded_batch.append(output_str)\n",
    "\n",
    "for i, seq in enumerate(decoded_batch):\n",
    "    print(f\"String {i}: {seq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at our generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String 0: e may be -> e may beg:yS??smx 3WiGMiBLd$m'iH?sNQC!3OSzKldX.Mmx;kQvn\n",
      "KGBtkQMcuUmINRn$I,t BhhMGdoCxIbEdLK\n",
      "eIfaX-gOI-YURdNT\n",
      "\n",
      "----------\n",
      "String 1: the ears -> the ears?TLo,rTH?zdNvs-qXy.iHVb.iv:B:r\n",
      "mcC:BqE!J:i.xbdY$IZxlWj!xikQ?BvESHfaQC:Q!BmIivJCCFuxXsHhrwaWILhXEJp-o\n",
      "\n",
      "----------\n",
      "String 2: ation? Y -> ation? YXRcatLGJx'3W;gXZw.\n",
      "'A3YCPbJyYBy3hNZ oTkL'$'c&&qLk-Xj!fAzp;r!.YsHpoELFi?ghNTtkOGJsxruJDEJSUk,tpv'VbMZ\n",
      "\n",
      "----------\n",
      "String 3: ore I ca -> ore I caH?-:aCpPWSCZ:ejBLGqis?g'Aoj exzzoQPPfGRlvXoJbu.RhlK?g:SHlM?zkcoagxSHSAxkRLilWCqRcj!aP-jUclqXMA33PluV\n",
      "\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "decoded_generated_text = []\n",
    "for seq in generated_tokens:\n",
    "    output_str = \"\"\n",
    "    for idx in seq:\n",
    "        output_str += itos[idx.item()]\n",
    "    decoded_generated_text.append(output_str)\n",
    "\n",
    "for i, (seq, generated_seq) in enumerate(\n",
    "    zip(decoded_batch, decoded_generated_text)\n",
    "):\n",
    "    print(f\"String {i}: {seq} -> {generated_seq}\\n\")\n",
    "    print('-' * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the strings that are generated are a bunch of random-looking characters. This is because we randomly generated the weights of our lookup table.\n",
    "\n",
    "Let's set up the logic to train our model then so that it becomes less random.\n",
    "\n",
    "We'll set up what the training logic will look like. The current results from training won't be very impressive, but we'll use the same logic and swap in more powerful training logic later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "batch_size = 32\n",
    "n_epochs = 100\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 4.737148761749268\n",
      "Epoch 10, Loss: 4.644645690917969\n",
      "Epoch 20, Loss: 4.661149501800537\n",
      "Epoch 30, Loss: 4.517314910888672\n",
      "Epoch 40, Loss: 4.539600372314453\n",
      "Epoch 50, Loss: 4.698057651519775\n",
      "Epoch 60, Loss: 4.696743011474609\n",
      "Epoch 70, Loss: 4.54141902923584\n",
      "Epoch 80, Loss: 4.665844440460205\n",
      "Epoch 90, Loss: 4.5253987312316895\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch(split=\"train\")\n",
    "\n",
    "    # evaluate loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our general logic. The loss doesn't go down by much here. Let's try to train for more iterations.\n",
    "\n",
    "Our training loop has the effect of updating the initial weights in our lookup table such that they'll more accurately reflect what we expect.\n",
    "\n",
    "Let's train this for many more iterations though since the update step is pretty simple, in order to see if we can get a more useful result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 4.638540267944336\n",
      "Epoch 1000, Loss: 3.658257246017456\n",
      "Epoch 2000, Loss: 3.0377516746520996\n",
      "Epoch 3000, Loss: 2.8597161769866943\n",
      "Epoch 4000, Loss: 2.5883102416992188\n",
      "Epoch 5000, Loss: 2.5793514251708984\n",
      "Epoch 6000, Loss: 2.4103808403015137\n",
      "Epoch 7000, Loss: 2.4814467430114746\n",
      "Epoch 8000, Loss: 2.4670302867889404\n",
      "Epoch 9000, Loss: 2.4843976497650146\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10000):\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch(split=\"train\")\n",
    "\n",
    "    # evaluate loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at our results.\n",
    "\n",
    "It's clear that even though the results are still pretty random, it's much better than the original results. It's beginning to at least take the structure and lengths that you expect in a Shakespeare play. Words are still pretty random but it looks a lot more like an encrypted message than a random string of characters. Heck, it even appears to get close to resembling some text.\n",
    "\n",
    "This is a promising approach, it's impressive that we can begin to get somewhat plausible text just from a simple bigram model combined with backpropagation. We don't even include any other context besides what character came earlier! During backpropagation, the gradients are computed for the selected rows in the embedding lookup table, and then the optimizer updates these rows based on the gradients. Across epochs, this has the effect of updating the rows of the embedding lookup table so that they more appropriately represent the probability of next characters given a certain reference index character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wen\n",
      "TI ks, s sen bance;\n",
      "Herd,Wh t!\n",
      "Yolooutto knd wroushilde, ay nd Whan;mod nd mer d ononisinat I Hegids o mapte t beve theamous invere, beloveters HARO,\n",
      "\n",
      "\n",
      "\n",
      "I aven he ft ouray o man:\n",
      "Thesujhaplis, m Lind tr anngredsowinavourlfat wathin k, sureind wahavan?\n",
      "DUSe wers k.\n",
      "ING s hil ard istis,\n",
      "Mavesh tsthe t muirey fave urcke by n EDI ss K:\n",
      "I:\n",
      "I murolengny fot ifothe,\n",
      "O:-al:\n",
      "Dondsuphenyookerou ucrcobr aly.\n",
      "Ancre dagbeausoroours den.Whillof plearr'laret RYCESwnte t, bishepatifay hatt gdan, waneve me c\n"
     ]
    }
   ],
   "source": [
    "print(decode(model.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make this much more powerful though if we can use more of the context than just what character came before the one that we're trying to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The mathematical trick in self-attention\n",
    "\n",
    "Let's consider the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape=torch.Size([4, 8, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4, 8, 2\n",
    "x = torch.randn(B, T, C)\n",
    "print(f\"{x.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, the 8 tokens in each sequence are not talking to each other. We want the tokens to talk to each other, but in a very specific way.\n",
    "\n",
    "For example, if we are at the 5th token, we want to use the 1st-4th tokens as context, but not any future tokens.\n",
    "\n",
    "What is an easy way to encapsulate the \"context\"? One naive way is to just take the average of the tokens before the $t^{th}$ token and use that as the context for the $t^{th}$ token.\n",
    "\n",
    "If I'm the 5th token, for example, I want to take the information from my time step but also steps 1-4 as well, and then take a weighted average of the those, in order to create a feature vector that \"summarizes\" the 5th token.\n",
    "\n",
    "This obviously loses a lot of information, especially positional information. This falls for a lot of the same flaws as, say, one-hot encoding or bag of words does, namely that you can't capture position. For example, \"the cat is jumping over the dog\" and \"the dog is jumping over the cat\" are treated identically because they have the same words. We take the \"average representation\" of all the words up to and including the $t^{th}$ token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow = torch.zeros((B,T,C)) # bag of words\n",
    "# for each sequence in the batch\n",
    "for b in range(B):\n",
    "    # for each token in the sequence\n",
    "    for t in range(T):\n",
    "        # get the previous tokens.\n",
    "        x_prev = x[b,:t+1] # (t, C)\n",
    "        # get the mean of the previous tokens\n",
    "        x_mean = x_prev.mean(dim=0)\n",
    "        # set the mean to the current token.\n",
    "        xbow[b,t] = x_mean\n",
    "\n",
    "# we want x[b,t] = mean_{i<=t} x[b,i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at how this looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xbow.shape=torch.Size([4, 8, 2])\n"
     ]
    }
   ],
   "source": [
    "print(f\"{xbow.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this looks for, say, the 1st sequence. We see that for the first sequence, we get a result with shape `[8,2]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x[0,:,:].shape=torch.Size([8, 2])\n"
     ]
    }
   ],
   "source": [
    "print(f\"{x[0,:,:].shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's figure out how we got the value at, say, `[0, 4,:]`, the 5th element in the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x[0,4,:]=tensor([0.3612, 1.1679])\n",
      "xbow[0,4,:]=tensor([0.3525, 0.0545])\n"
     ]
    }
   ],
   "source": [
    "print(f\"{x[0,4,:]=}\") # original value\n",
    "print(f\"{xbow[0,4,:]=}\") # averaged value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the elements are from the 1st to the 5th elements and then average them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x[0,4,:]=tensor([0.3612, 1.1679])\n"
     ]
    }
   ],
   "source": [
    "print(f\"{x[0,4,:]=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x[0,0:5,:]=tensor([[ 0.1808, -0.0700],\n",
      "        [-0.3596, -0.9152],\n",
      "        [ 0.6258,  0.0255],\n",
      "        [ 0.9545,  0.0643],\n",
      "        [ 0.3612,  1.1679]])\n",
      "x[0,:5].mean(dim=0)=tensor([0.3525, 0.0545])\n"
     ]
    }
   ],
   "source": [
    "print(f\"{x[0,0:5,:]=}\")\n",
    "print(f\"{x[0,:5].mean(dim=0)=}\") # matches our averaged value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We took the logits of the 1st to 5th values, averaged them out, and that was our representation for the 5th value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The trick: migrating this to matrix multiplication\n",
    "We can actually do this quite efficiently if we use matrix multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this can work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "a.shape=torch.Size([3, 3])\n",
      "b=tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "b.shape=torch.Size([3, 2])\n",
      "c=tensor([[14., 16.],\n",
      "        [14., 16.],\n",
      "        [14., 16.]])\n",
      "c.shape=torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.ones((3, 3))\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "c = a @ b\n",
    "print(f\"{a=}\")\n",
    "print(f\"{a.shape=}\")\n",
    "print(f\"{b=}\")\n",
    "print(f\"{b.shape=}\")\n",
    "print(f\"{c=}\")\n",
    "print(f\"{c.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplying by a matrix of ones naturally adds all of the elements per column:\n",
    "\n",
    "$$\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "1&1&1\\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "*\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "2\\\\\n",
    "6\\\\\n",
    "6\\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "= 14\n",
    "\\rightarrow\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "1&1&1\\\\\n",
    "1&1&1\\\\\n",
    "1&1&1\\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "*\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "2\\\\\n",
    "6\\\\\n",
    "6\\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "=\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "14\\\\\n",
    "14\\\\\n",
    "14\\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "1&1&1\\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "*\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "7\\\\\n",
    "4\\\\\n",
    "5\\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "= 16\n",
    "\\rightarrow\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "1&1&1\\\\\n",
    "1&1&1\\\\\n",
    "1&1&1\\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "*\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "7\\\\\n",
    "4\\\\\n",
    "5\\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "=\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "16\\\\\n",
    "16\\\\\n",
    "16\\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "We can then simply average these, if we wanted, by taking any of the values per column and dividing by the mean:\n",
    "$$\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "\\frac{14}{3}&\\frac{16}{3}\\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "We've now established that we can easily get the sums of a tensor via matrix multiplication.\n",
    "\n",
    "Let's now look at another piece for the calculation: `torch.tril`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lower triangular matrix\n",
    "torch.tril(torch.ones(3,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's replicate our logic from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tril(torch.ones(3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "b=tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "c=tensor([[ 2.,  7.],\n",
      "        [ 8., 11.],\n",
      "        [14., 16.]])\n",
      "c.shape=torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "c = a @ b\n",
    "print(f\"{a=}\")\n",
    "print(f\"{b=}\")\n",
    "print(f\"{c=}\")\n",
    "print(f\"{c.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down what we're looking at by looking at the output matrix.\n",
    "\n",
    "The first output row is $\\left[2,7\\right]$. We get this through the matrix multiplication of:\n",
    "$$\n",
    "\\left[\n",
    "1,0,0\n",
    "\\right]\n",
    "*\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "2&7\\\\\n",
    "6&4\\\\\n",
    "6&5\\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "\\rightarrow\n",
    "\\left[(1*2 + 0*6 + 0*6),(1*7 + 0*4 + 0*5)\\right]\n",
    "\\rightarrow\n",
    "\\left[2, 7\\right]\n",
    "$$\n",
    "\n",
    "We take the sum of the first element and zero out the rest.\n",
    "\n",
    "The second output row is $\\left[8, 11\\right]$. We get this through the matrix multiplication of:\n",
    "$$\n",
    "\\left[\n",
    "1,1,0\n",
    "\\right]\n",
    "*\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "2&7\\\\\n",
    "6&4\\\\\n",
    "6&5\\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "\\rightarrow\n",
    "\\left[(1*2 + 1*6 + 0*6),(1*7 + 1*4 + 0*5)\\right]\n",
    "\\rightarrow\n",
    "\\left[8, 11\\right]\n",
    "$$\n",
    "\n",
    "We calculate the second row by suming the first and second elements and zeroing out the rest.\n",
    "\n",
    "The same logic applies for the third row.\n",
    "\n",
    "By multiplying by the triangular matrix of ones, we can get the cumulative sum, at index $t$, of all elements up to and including $t$, which is exactly what we had done with a for loop before. This lets us perform the operation of \"get the sum of all the previous terms up to the term at position $t$\" in a mathematical way.\n",
    "\n",
    "Now, to calculate the means, we can just change $a$ so that we normalize across all the rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a before normalization: tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "a after normalization: tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tril(torch.ones(3,3))\n",
    "print(f\"a before normalization: {a}\")\n",
    "a = a / a.sum(dim=-1, keepdim=True)\n",
    "print(f\"a after normalization: {a}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural-networks-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

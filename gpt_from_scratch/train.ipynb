{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT from scratch\n",
    "\n",
    "Follows [this](https://www.youtube.com/watch?v=kCc8FmEb1nY) video tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !curl -O https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of text: {len(text)}\")\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "vocab = {c: i for i, c in enumerate(chars)}\n",
    "print(len(vocab)) # 65 unique chars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to now be able to tokenize the text somehow. Here, we're building a character-level model, so we're just translating individual characters into integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {c: i for i, c in enumerate(chars)}\n",
    "itos = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "# take a string, return a list of integers\n",
    "encode = lambda x: [stoi[c] for c in x]\n",
    "# take a list of integers, return a string\n",
    "decode = lambda x: ''.join([itos[i] for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 43, 50, 50, 53, 1, 58, 46, 47, 57, 1, 47, 57, 1, 39, 1, 58, 43, 57, 58, 1, 57, 58, 56, 47, 52, 45]\n",
      "Hello this is a test string\n"
     ]
    }
   ],
   "source": [
    "print(encode(\"Hello this is a test string\"))\n",
    "print(decode(encode(\"Hello this is a test string\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now tokenize the entire text dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's also split out data into train and validation datasets\n",
    "n = int(0.9*len(data))\n",
    "train_data, val_data = data[:n], data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't pass in the entire dataset in one iteration of the transformer, so we do need to set a \"block size\" (AKA context length)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at one block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training using chunks of the context window\n",
    "A transformer makes simultaneous predictions for each of these positions. This makes training much more efficient since we treat each position as a separate prediction task instead of just predicting the next character after the last position, which greatly increases the number of samples used for training.\n",
    "\n",
    "This is why we take `[:block_size+1]` since if we have 8 chars, we need to look 9 chars so that we can always predict the char after a given char.\n",
    "\n",
    "We can actually display what the prediction task is: given a block of text, we predict what token is at each position. The input passed into each prediction task is whatever tokens were in the block before this target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is tensor([18]), the target is: 47\n",
      "When input is tensor([18, 47]), the target is: 56\n",
      "When input is tensor([18, 47, 56]), the target is: 57\n",
      "When input is tensor([18, 47, 56, 57]), the target is: 58\n",
      "When input is tensor([18, 47, 56, 57, 58]), the target is: 1\n",
      "When input is tensor([18, 47, 56, 57, 58,  1]), the target is: 15\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15]), the target is: 47\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47]), the target is: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"When input is {context}, the target is: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in a chunk of 9 characters, we essentially have 8 training examples. This not only gives us more training samples, but it also makes the Transformer used to seeing context sizes from as little as 1 all the way to the context length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching\n",
    "\n",
    "We also need to care about the batching dimension as well, since we'll be feeding the input as batches of tensors. This lets us process more inputs in parallel, especially since GPUs are great at working with batches of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10f29b030>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of sequences in a batch, processed in parallel.\n",
    "batch_size = 4\n",
    "\n",
    "# maximum context length for prediction\n",
    "block_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    \"\"\"Gets a batch of data for training or validation.\"\"\"\n",
    "    data = (\n",
    "        train_data if split == \"train\" else val_data\n",
    "    )\n",
    "    # we take random ints as our start indices. We make\n",
    "    # sure to take random ints that allow us to take a full\n",
    "    # sequence of `block_size` length\n",
    "    ix = torch.randint(\n",
    "        len(data) - block_size, # which ints to include in pool\n",
    "        (batch_size,) # number of results = batch size.\n",
    "    )\n",
    "\n",
    "    # for each start index, we take a random block of chars.\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb = get_batch(split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at our batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0\n",
      "----------\n",
      "When input is tensor([24]), the target is: 43\n",
      "When input is tensor([24, 43]), the target is: 58\n",
      "When input is tensor([24, 43, 58]), the target is: 5\n",
      "When input is tensor([24, 43, 58,  5]), the target is: 57\n",
      "When input is tensor([24, 43, 58,  5, 57]), the target is: 1\n",
      "When input is tensor([24, 43, 58,  5, 57,  1]), the target is: 46\n",
      "When input is tensor([24, 43, 58,  5, 57,  1, 46]), the target is: 43\n",
      "When input is tensor([24, 43, 58,  5, 57,  1, 46, 43]), the target is: 39\n",
      "----------\n",
      "Batch 1\n",
      "----------\n",
      "When input is tensor([44]), the target is: 53\n",
      "When input is tensor([44, 53]), the target is: 56\n",
      "When input is tensor([44, 53, 56]), the target is: 1\n",
      "When input is tensor([44, 53, 56,  1]), the target is: 58\n",
      "When input is tensor([44, 53, 56,  1, 58]), the target is: 46\n",
      "When input is tensor([44, 53, 56,  1, 58, 46]), the target is: 39\n",
      "When input is tensor([44, 53, 56,  1, 58, 46, 39]), the target is: 58\n",
      "When input is tensor([44, 53, 56,  1, 58, 46, 39, 58]), the target is: 1\n",
      "----------\n",
      "Batch 2\n",
      "----------\n",
      "When input is tensor([52]), the target is: 58\n",
      "When input is tensor([52, 58]), the target is: 1\n",
      "When input is tensor([52, 58,  1]), the target is: 58\n",
      "When input is tensor([52, 58,  1, 58]), the target is: 46\n",
      "When input is tensor([52, 58,  1, 58, 46]), the target is: 39\n",
      "When input is tensor([52, 58,  1, 58, 46, 39]), the target is: 58\n",
      "When input is tensor([52, 58,  1, 58, 46, 39, 58]), the target is: 1\n",
      "When input is tensor([52, 58,  1, 58, 46, 39, 58,  1]), the target is: 46\n",
      "----------\n",
      "Batch 3\n",
      "----------\n",
      "When input is tensor([25]), the target is: 17\n",
      "When input is tensor([25, 17]), the target is: 27\n",
      "When input is tensor([25, 17, 27]), the target is: 10\n",
      "When input is tensor([25, 17, 27, 10]), the target is: 0\n",
      "When input is tensor([25, 17, 27, 10,  0]), the target is: 21\n",
      "When input is tensor([25, 17, 27, 10,  0, 21]), the target is: 1\n",
      "When input is tensor([25, 17, 27, 10,  0, 21,  1]), the target is: 54\n",
      "When input is tensor([25, 17, 27, 10,  0, 21,  1, 54]), the target is: 39\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for b in range(batch_size):\n",
    "    print(f\"Batch {b}\")\n",
    "    print('-' * 10)\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"When input is {context}, the target is: {target}\")\n",
    "    print('-' * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these batch samples becomes a row in the data. What we end up getting is a `(batch_size, block_size)`-shaped tensor for our training data.For batch_size=4 and block_size=8, this leads to a 4x8 tensor.\n",
    "\n",
    "Because we predict the next token for each position in the block, we get 8 training samples out of a block of size=8. This means that in this batch, we have 32 training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting neural network training with the simplest model: bigrams\n",
    "Now we can start using this input for neural network training. We can start with the simplest model: the bigram.\n",
    "\n",
    "#### What is a bigram model?\n",
    "A bigram model is a model that predicts the next word given the previous word in a sentence. It assumes that the probability of the next word is *only affected by the previous word*.\n",
    "\n",
    "For our case, we'll use a bigram character model, where the probability of a given character is based only on the probability of the previous character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10f29b030>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    \"\"\"Bigram model.\"\"\"\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # read off the logits for the next token\n",
    "        # from a lookup table.\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "    \n",
    "    def forward(self, idx):\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        # get the logits for the next token.\n",
    "        # for a given batch with shape (B, T), we will get an output\n",
    "        # of shape (B, T, C), where B = batch size, T = sequence length\n",
    "        # and C = vocab size.\n",
    "        logits = self.token_embedding_table(idx)\n",
    "        return logits\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"Predict the next token.\"\"\"\n",
    "        # get the logits\n",
    "        logits = self(x)\n",
    "        # get argmax\n",
    "        return logits.argmax(dim=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this would look so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.token_embedding_table.weight.shape=torch.Size([65, 65])\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel(vocab_size=vocab_size)\n",
    "print(f\"{model.token_embedding_table.weight.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a naive prediction. Given a particular index in our vocabulary, let's see what the predicted output is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input index: 5, Prediction index: 39\n",
      "Output logits shape: torch.Size([1, 65])\n",
      "Input: ', Prediction: a\n"
     ]
    }
   ],
   "source": [
    "test_idx = 5\n",
    "logits = model(torch.tensor([test_idx]))\n",
    "prediction = model.predict(torch.tensor([test_idx]))\n",
    "print(f\"Input index: {test_idx}, Prediction index: {prediction.item()}\")\n",
    "# tensor of shape (1, vocab_size), with each element being the\n",
    "# logit for each token in the vocabulary at the given index.\n",
    "print(f\"Output logits shape: {logits.shape}\")\n",
    "# we take the max index of the logits to get the\n",
    "# predicted token.\n",
    "print(f\"Input: {itos[test_idx]}, Prediction: {itos[prediction.item()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it on a batch of texts. Let's get the accuracy based on a random set of results. When we run `forward`, we get a result of shape `(batch_size, block_size, vocab_size)`. We get a `[vocab_size]`-shaped tensor for all positions in our `[batch_size, block_size]`-shaped batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_logits = model(xb)\n",
    "preds = model.predict(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_logits.shape=torch.Size([4, 8, 65])\n",
      "preds.shape=torch.Size([4, 8])\n",
      "yb.shape=torch.Size([4, 8])\n",
      "Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "# batch logits shape = (batch_size, block_size, vocab_size)\n",
    "print(f\"{batch_logits.shape=}\")\n",
    "\n",
    "# preds shape = (batch_size, block_size) because we\n",
    "# take the argmax on the -1 dim to get the predicted token.\n",
    "print(f\"{preds.shape=}\")\n",
    "print(f\"{yb.shape=}\")\n",
    "print(f\"Accuracy: {(preds == yb).float().mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model only has an accuracy of 3%. Again, this makes sense given that our model is randomly initialized.\n",
    "\n",
    "Of course this doesn't have any actual basis, since this is based on a randomly initialized lookup table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add a loss function to our model so we can have a way to evaluate \"how good\" the predictions are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    \"\"\"Bigram model.\"\"\"\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # read off the logits for the next token\n",
    "        # from a lookup table.\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        # get the logits for the next token.\n",
    "        # for a given batch with shape (B, T), we will get an output\n",
    "        # of shape (B, T, C), where B = batch size, T = sequence length\n",
    "        # and C = vocab size.\n",
    "        logits = self.token_embedding_table(idx)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            # flatten the logits\n",
    "            logits = logits.view(B*T, C)\n",
    "            # flatten the targets\n",
    "            targets = targets.view(B*T)\n",
    "            # compute the loss\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"Predict the next token.\"\"\"\n",
    "        # get the logits\n",
    "        logits = self(x)\n",
    "        # get argmax\n",
    "        return logits.argmax(dim=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get the predictions and loss. We get the logits for all `[batch_size, block_size]` characters as well as the total loss.\n",
    "\n",
    "We're expecting something like `-ln(1/65) ~ 4.17`, which is what the loss would be if we were accurate `1/65` of the time (which is expected for a random model with vocab size of 65)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits.shape=torch.Size([32, 65])\n",
      "loss=tensor(4.6630, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel(vocab_size=vocab_size)\n",
    "logits, loss = model(xb, yb)\n",
    "print(f\"{logits.shape=}\")\n",
    "print(f\"{loss=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can evaluate the loss, we'd like to also be able to do generation from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    \"\"\"Bigram model.\"\"\"\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # read off the logits for the next token\n",
    "        # from a lookup table.\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        # get the logits for the next token.\n",
    "        # for a given batch with shape (B, T), we will get an output\n",
    "        # of shape (B, T, C), where B = batch size, T = sequence length\n",
    "        # and C = vocab size.\n",
    "        logits = self.token_embedding_table(idx)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            # flatten the logits\n",
    "            logits = logits.view(B*T, C)\n",
    "            # flatten the targets\n",
    "            targets = targets.view(B*T)\n",
    "            # compute the loss\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"Predict the next token.\"\"\"\n",
    "        # get the logits\n",
    "        logits = self(x)\n",
    "        # get argmax\n",
    "        return logits.argmax(dim=-1)\n",
    "\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        \"\"\"Generate new tokens.\n",
    "        \n",
    "        idx is (B,T) shaped tensor with array of indices\n",
    "        that are in the current context.\n",
    "\n",
    "        At each step, we predict the next token and add it to the context.\n",
    "\n",
    "        Algorithm:\n",
    "            - We get the logits for the last token in the context for each sequence\n",
    "            in the batch\n",
    "            - We apply softmax to get probabilities for the next token, for each\n",
    "            sequence in the batch.\n",
    "            - We sample from the distribution to get the next token for each sequence\n",
    "            in the batch.\n",
    "            - We add the new token to the context and repeat.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get predictions\n",
    "            logits, _ = self(idx)\n",
    "            # focus on last time stamp\n",
    "            # becomes (B, C) since we take the logits of the\n",
    "            # last position in each sequence of the batch.\n",
    "            logits = logits[:, -1, :]\n",
    "            # apply softmax to get probabilities.\n",
    "            # (B, C), but now values are probabilities.\n",
    "            # Each row is a probability distribution whose values add\n",
    "            # up to 1.\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from the distribution\n",
    "            # (B, 1). We get the next token for each sample in the batch.\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            # we add the new token to the context and repeat.\n",
    "            # idx originally (B, T), next_token (B, 1), so when we concatenate\n",
    "            # we get (B, T+1)\n",
    "            idx = torch.cat([idx, next_token], dim=1) # (B, T+1)\n",
    "\n",
    "        # shape of idx is (B, T+max_new_tokens) since we've added\n",
    "        # max_new_tokens to the context.\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BigramLanguageModel(vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xb.shape=torch.Size([4, 8])\n",
      "generated_tokens.shape=torch.Size([4, 108])\n"
     ]
    }
   ],
   "source": [
    "max_new_tokens = 100\n",
    "generated_tokens = model.generate(xb, max_new_tokens)\n",
    "# batch_size of 4\n",
    "# 8 initial tokens\n",
    "# => (4, 8)\n",
    "print(f\"{xb.shape=}\")\n",
    "# batch_size of 4\n",
    "# for each sequence, 8 initial tokens + 100 generated tokens.\n",
    "# => (4, 108)\n",
    "print(f\"{generated_tokens.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's decode both of these to see the actual letters. First, let's look at our batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String 0: Let's he\n",
      "String 1: for that\n",
      "String 2: nt that \n",
      "String 3: MEO:\n",
      "I p\n"
     ]
    }
   ],
   "source": [
    "decoded_batch = []\n",
    "for seq in xb:\n",
    "    output_str = \"\"\n",
    "    for idx in seq:\n",
    "        output_str += itos[idx.item()]\n",
    "    decoded_batch.append(output_str)\n",
    "\n",
    "for i, seq in enumerate(decoded_batch):\n",
    "    print(f\"String {i}: {seq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at our generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String 0: Let's he -> Let's heO;$XXId TuoJck,rsLYMOY:HA!;UQZMAOBt&oNk,.?Cn.$pXWKCaL3uc.Y:lsL3IKcSjMWixZfhqVQZYjJpKF!lcE-HHsejq$ntj\n",
      "\n",
      "----------\n",
      "String 1: for that -> for thatwckIGObSYrDZOEgqxYDjeRjEISpDSsu\n",
      "dFV:PDBOkCBhS,.zk'.!:TfrWx$\n",
      "dXHStdYKt?aMmQuaOl;UobB;MWi?V-HFYx, !wtd\n",
      "\n",
      "----------\n",
      "String 2: nt that  -> nt that WyZvFrk,,,i:thNdlBJZIVNSqPe,,KaHtBIE?Gc!\n",
      "y:TtMwKbL:FBt!H$CfVDSjY&$hzGcbSoc,HbKAcp'tBR'wLGck.jC'H,llM\n",
      "\n",
      "----------\n",
      "String 3: MEO:\n",
      "I p -> MEO:\n",
      "I piEgWKckxrZIckeIhlk,.JOHAa?kYauc.ErucA R,BAy?bECGmX?QJMLFFmNdXGBRfRUH;U'cA.YxJuZE$leb&I;Z3VqTK&-BqWBz\n",
      "\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "decoded_generated_text = []\n",
    "for seq in generated_tokens:\n",
    "    output_str = \"\"\n",
    "    for idx in seq:\n",
    "        output_str += itos[idx.item()]\n",
    "    decoded_generated_text.append(output_str)\n",
    "\n",
    "for i, (seq, generated_seq) in enumerate(\n",
    "    zip(decoded_batch, decoded_generated_text)\n",
    "):\n",
    "    print(f\"String {i}: {seq} -> {generated_seq}\\n\")\n",
    "    print('-' * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the strings that are generated are a bunch of random-looking characters. This is because we randomly generated the weights of our lookup table.\n",
    "\n",
    "Let's set up the logic to train our model then so that it becomes less random.\n",
    "\n",
    "We'll set up what the training logic will look like. The current results from training won't be very impressive, but we'll use the same logic and swap in more powerful training logic later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "batch_size = 32\n",
    "n_epochs = 100\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 4.714339733123779\n",
      "Epoch 10, Loss: 4.676767349243164\n",
      "Epoch 20, Loss: 4.701565265655518\n",
      "Epoch 30, Loss: 4.765804290771484\n",
      "Epoch 40, Loss: 4.667387008666992\n",
      "Epoch 50, Loss: 4.631993293762207\n",
      "Epoch 60, Loss: 4.670619487762451\n",
      "Epoch 70, Loss: 4.655516624450684\n",
      "Epoch 80, Loss: 4.576014995574951\n",
      "Epoch 90, Loss: 4.6416544914245605\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch(split=\"train\")\n",
    "\n",
    "    # evaluate loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our general logic. The loss doesn't go down by much here. Let's try to train for more iterations.\n",
    "\n",
    "Our training loop has the effect of updating the initial weights in our lookup table such that they'll more accurately reflect what we expect.\n",
    "\n",
    "Let's train this for many more iterations though since the update step is pretty simple, in order to see if we can get a more useful result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 4.506431579589844\n",
      "Epoch 1000, Loss: 3.6920104026794434\n",
      "Epoch 2000, Loss: 3.0407776832580566\n",
      "Epoch 3000, Loss: 2.6942100524902344\n",
      "Epoch 4000, Loss: 2.6889488697052\n",
      "Epoch 5000, Loss: 2.5391829013824463\n",
      "Epoch 6000, Loss: 2.5220439434051514\n",
      "Epoch 7000, Loss: 2.5135819911956787\n",
      "Epoch 8000, Loss: 2.623948335647583\n",
      "Epoch 9000, Loss: 2.479816198348999\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10000):\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch(split=\"train\")\n",
    "\n",
    "    # evaluate loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at our results.\n",
    "\n",
    "It's clear that even though the results are still pretty random, it's much better than the original results. It's beginning to at least take the structure and lengths that you expect in a Shakespeare play. Words are still pretty random but it looks a lot more like an encrypted message than a random string of characters. Heck, it even appears to get close to resembling some text.\n",
    "\n",
    "This is a promising approach, it's impressive that we can begin to get somewhat plausible text just from a simple bigram model combined with backpropagation. We don't even include any other context besides what character came earlier! During backpropagation, the gradients are computed for the selected rows in the embedding lookup table, and then the optimizer updates these rows based on the gradients. Across epochs, this has the effect of updating the rows of the embedding lookup table so that they more appropriately represent the probability of next characters given a certain reference index character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "He bldese bad f anthtourmand rseno H:\n",
      "d wel.\n",
      "Se, theananonge!\n",
      "I, t the ksoth rmorad'Thathany tw t theareace 'K:\n",
      "Bur f mowetooutwis K:\n",
      "QUS:\n",
      "Yo one glle:\n",
      "TOKITESe imo'd y al intr ancV:\n",
      "ERCHeeanthoulallian, n, t, w;\n",
      "Q:\n",
      "\n",
      "AMENCAndoound I oyofe.\n",
      "Anl ouke givapat be meat,-kers,\n",
      "CHARI shears m gmavencl hs\n",
      "S:\n",
      "ABo e t f I'dYBEETIne, s theom gr tenouthonomoreand by maw cextht,VI,\n",
      "Sig I pENGI r frtite s t brse milddor iv, wind,\n",
      "Hano, t,\n",
      "\n",
      "'s ethace hekanoure saico heneef m:\n",
      "Whed nce INThe llas'd Bl oungollto\n"
     ]
    }
   ],
   "source": [
    "print(decode(model.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make this much more powerful though if we can use more of the context than just what character came before the one that we're trying to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The mathematical trick in self-attention\n",
    "\n",
    "Let's consider the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape=torch.Size([4, 8, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4, 8, 2\n",
    "x = torch.randn(B, T, C)\n",
    "print(f\"{x.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, the 8 tokens in each sequence are not talking to each other. We want the tokens to talk to each other, but in a very specific way.\n",
    "\n",
    "For example, if we are at the 5th token, we want to use the 1st-4th tokens as context, but not any future tokens.\n",
    "\n",
    "What is an easy way to encapsulate the \"context\"? One naive way is to just take the average of the tokens before the $t^{th}$ token and use that as the context for the $t^{th}$ token.\n",
    "\n",
    "If I'm the 5th token, for example, I want to take the information from my time step but also steps 1-4 as well, and then take a weighted average of the those, in order to create a feature vector that \"summarizes\" the 5th token.\n",
    "\n",
    "This obviously loses a lot of information, especially positional information. This falls for a lot of the same flaws as, say, one-hot encoding or bag of words does, namely that you can't capture position. For example, \"the cat is jumping over the dog\" and \"the dog is jumping over the cat\" are treated identically because they have the same words. We take the \"average representation\" of all the words up to and including the $t^{th}$ token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow = torch.zeros((B,T,C)) # bag of words\n",
    "# for each sequence in the batch\n",
    "for b in range(B):\n",
    "    # for each token in the sequence\n",
    "    for t in range(T):\n",
    "        # get the previous tokens.\n",
    "        x_prev = x[b,:t+1] # (t, C)\n",
    "        # get the mean of the previous tokens\n",
    "        x_mean = x_prev.mean(dim=0)\n",
    "        # set the mean to the current token.\n",
    "        xbow[b,t] = x_mean\n",
    "\n",
    "# we want x[b,t] = mean_{i<=t} x[b,i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at how this looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xbow.shape=torch.Size([4, 8, 2])\n"
     ]
    }
   ],
   "source": [
    "print(f\"{xbow.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this looks for, say, the 1st sequence. We see that for the first sequence, we get a result with shape `[8,2]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x[0,:,:].shape=torch.Size([8, 2])\n"
     ]
    }
   ],
   "source": [
    "print(f\"{x[0,:,:].shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's figure out how we got the value at, say, `[0, 4,:]`, the 5th element in the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x[0,4,:]=tensor([0.3612, 1.1679])\n",
      "xbow[0,4,:]=tensor([0.3525, 0.0545])\n"
     ]
    }
   ],
   "source": [
    "print(f\"{x[0,4,:]=}\") # original value\n",
    "print(f\"{xbow[0,4,:]=}\") # averaged value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the elements are from the 1st to the 5th elements and then average them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x[0,4,:]=tensor([0.3612, 1.1679])\n"
     ]
    }
   ],
   "source": [
    "print(f\"{x[0,4,:]=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x[0,0:5,:]=tensor([[ 0.1808, -0.0700],\n",
      "        [-0.3596, -0.9152],\n",
      "        [ 0.6258,  0.0255],\n",
      "        [ 0.9545,  0.0643],\n",
      "        [ 0.3612,  1.1679]])\n",
      "x[0,:5].mean(dim=0)=tensor([0.3525, 0.0545])\n"
     ]
    }
   ],
   "source": [
    "print(f\"{x[0,0:5,:]=}\")\n",
    "print(f\"{x[0,:5].mean(dim=0)=}\") # matches our averaged value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We took the logits of the 1st to 5th values, averaged them out, and that was our representation for the 5th value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The trick: migrating this to matrix multiplication\n",
    "We can actually do this quite efficiently if we use matrix multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this can work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "a.shape=torch.Size([3, 3])\n",
      "b=tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "b.shape=torch.Size([3, 2])\n",
      "c=tensor([[14., 16.],\n",
      "        [14., 16.],\n",
      "        [14., 16.]])\n",
      "c.shape=torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.ones((3, 3))\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "c = a @ b\n",
    "print(f\"{a=}\")\n",
    "print(f\"{a.shape=}\")\n",
    "print(f\"{b=}\")\n",
    "print(f\"{b.shape=}\")\n",
    "print(f\"{c=}\")\n",
    "print(f\"{c.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplying by a matrix of ones naturally adds all of the elements per column:\n",
    "\n",
    "$$\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "1&1&1\\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "*\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "2\\\\\n",
    "6\\\\\n",
    "6\\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "= 14\n",
    "\\rightarrow\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "1&1&1\\\\\n",
    "1&1&1\\\\\n",
    "1&1&1\\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "*\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "2\\\\\n",
    "6\\\\\n",
    "6\\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "=\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "14\\\\\n",
    "14\\\\\n",
    "14\\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "1&1&1\\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "*\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "7\\\\\n",
    "4\\\\\n",
    "5\\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "= 16\n",
    "\\rightarrow\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "1&1&1\\\\\n",
    "1&1&1\\\\\n",
    "1&1&1\\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "*\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "7\\\\\n",
    "4\\\\\n",
    "5\\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "=\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "16\\\\\n",
    "16\\\\\n",
    "16\\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "We can then simply average these, if we wanted, by taking any of the values per column and dividing by the mean:\n",
    "$$\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "\\frac{14}{3}&\\frac{16}{3}\\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "We've now established that we can easily get the sums of a tensor via matrix multiplication.\n",
    "\n",
    "Let's now look at another piece for the calculation: `torch.tril`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lower triangular matrix\n",
    "torch.tril(torch.ones(3,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's replicate our logic from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tril(torch.ones(3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "b=tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "c=tensor([[ 2.,  7.],\n",
      "        [ 8., 11.],\n",
      "        [14., 16.]])\n",
      "c.shape=torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "c = a @ b\n",
    "print(f\"{a=}\")\n",
    "print(f\"{b=}\")\n",
    "print(f\"{c=}\")\n",
    "print(f\"{c.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down what we're looking at by looking at the output matrix.\n",
    "\n",
    "The first output row is $\\left[2,7\\right]$. We get this through the matrix multiplication of:\n",
    "$$\n",
    "\\left[\n",
    "1,0,0\n",
    "\\right]\n",
    "*\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "2&7\\\\\n",
    "6&4\\\\\n",
    "6&5\\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "\\rightarrow\n",
    "\\left[(1*2 + 0*6 + 0*6),(1*7 + 0*4 + 0*5)\\right]\n",
    "\\rightarrow\n",
    "\\left[2, 7\\right]\n",
    "$$\n",
    "\n",
    "We take the sum of the first element and zero out the rest.\n",
    "\n",
    "The second output row is $\\left[8, 11\\right]$. We get this through the matrix multiplication of:\n",
    "$$\n",
    "\\left[\n",
    "1,1,0\n",
    "\\right]\n",
    "*\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "2&7\\\\\n",
    "6&4\\\\\n",
    "6&5\\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "\\rightarrow\n",
    "\\left[(1*2 + 1*6 + 0*6),(1*7 + 1*4 + 0*5)\\right]\n",
    "\\rightarrow\n",
    "\\left[8, 11\\right]\n",
    "$$\n",
    "\n",
    "We calculate the second row by suming the first and second elements and zeroing out the rest.\n",
    "\n",
    "The same logic applies for the third row.\n",
    "\n",
    "By multiplying by the triangular matrix of ones, we can get the cumulative sum, at index $t$, of all elements up to and including $t$, which is exactly what we had done with a for loop before. This lets us perform the operation of \"get the sum of all the previous terms up to the term at position $t$\" in a mathematical way.\n",
    "\n",
    "Now, to calculate the means, we can just change $a$ so that we normalize across all the rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a before normalization: tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "a after normalization: tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tril(torch.ones(3,3))\n",
    "print(f\"a before normalization: {a}\")\n",
    "a = a / a.sum(dim=-1, keepdim=True)\n",
    "print(f\"a after normalization: {a}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matrix will now let us calculate \"add up all the previous terms up to and including the term at position $t$ **and** find the mean\" in a mathematical way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "b=tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "c=tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "c = a @ b\n",
    "print(f\"{a=}\")\n",
    "print(f\"{b=}\")\n",
    "print(f\"{c=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now replaced the previous for-loop with a matrix operation.\n",
    "\n",
    "Let's see what else we can vectorize to make things even more efficient.\n",
    "\n",
    "We can generalize this to an arbitrary context length/block size $T$, which is what we've been using.\n",
    "\n",
    "If we do the same matrix multiplication for our arbitrary sample batch $x$, we see what we get the same results as our previous for-loop, generalized to a matrix multiplication. By multiplying with a normalized lower triangular matrix, we get the same weighted aggregation that we previously got with a for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = torch.tril(torch.ones(T, T)) # our weight matrix\n",
    "wei = wei / wei.sum(1, keepdim=True) # normalize the weights\n",
    "# originally, wei = (T, T)\n",
    "# PyTorch will make it (1, T, T)\n",
    "# it will then broadcast it to (B, T, T)\n",
    "# then, for each batch element, it will multiply (T,T) @ (T,C) -> (T,C)\n",
    "# so the output will be (B, T, C)\n",
    "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 8])\n",
      "torch.Size([4, 8, 2])\n",
      "torch.Size([4, 8, 2])\n"
     ]
    }
   ],
   "source": [
    "print(wei.shape)\n",
    "print(x.shape)\n",
    "print(xbow2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now rewrite it in one more way, which is to use the softmax. We use the same logic as before, but instead of having zeros, we use -inf and take the softmax of the weight matrix.\n",
    "\n",
    "By taking the softmax, we make sure that the rows all add up to 1, therefore creating a probability distribution. This will be useful for us since in practice, the weights won't add up to 1. We want to be able to interpret the weights as probabilities for what we're doing later, and taking the softmax helps us with that.\n",
    "\n",
    "This is also why we use -inf instead of 0, since the softmax of -inf is 0 ($e^{-\\infty} = 0$) while the softmax of 0 is 1 ($e^{0} = 1$), and we don't want 0 values to have any weight in the probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wei before: tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "wei after fill: tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "wei after softmax: tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T, T))\n",
    "print(f\"wei before: {wei}\")\n",
    "wei = wei.masked_fill(tril == 0, float(\"-inf\"))\n",
    "print(f\"wei after fill: {wei}\")\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "# all the zeros will have equal weighting, while -inf will have 0 weighting.\n",
    "print(f\"wei after softmax: {wei}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    }
   ],
   "source": [
    "print(wei)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recreated the same weight matrix as earlier, so let's do the same computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow, xbow3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we do attention, our weights will give a relative weighting of \"how much do we want to attend to that specific context\", and setting future tokens to -inf means \"don't attend to this token\". These weighing will be trained by the data so that certain rows can \"attend\" to other rows.\n",
    "\n",
    "We can use this lower triangular matrix + softmax method to do weighted aggregations of past elements so that we can have relative weightings of how important certain past tokens are to the understanding of the current token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up self-attention\n",
    "\n",
    "Now that we've set up that math, we can now start to construct self-attention.\n",
    "\n",
    "First, let's start by updating our Bigram model definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    \"\"\"Bigram model.\"\"\"\n",
    "    def __init__(self, vocab_size=65, block_size=8, n_embed=32):\n",
    "        super().__init__()\n",
    "        # we add a n_embed parameter to the model. This will tell us\n",
    "        # the embedding dimension of the tokens.\n",
    "        # we'll have 32-d embeddings for each token.\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "\n",
    "        # we also want to represent the position of whatever token that we're \n",
    "        # looking at. We'll use a positional embedding for this.\n",
    "        self.positional_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "\n",
    "        # language model head. Converts from token embedding to logits.\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        B, T = idx.shape\n",
    "        # now, instead of logits, we'll get token embeddings.\n",
    "        token_embeddings = self.token_embedding_table(idx) # (B, T, n_embed)\n",
    "        # we now also get the positional embeddings. We fetch the positional embeddings\n",
    "        # for each token in the sequence, up to length \"T\" (the sequence length).\n",
    "        # this helps us capture the position of each token in the sequence.\n",
    "        positions = torch.arange(T, device=idx.device)\n",
    "        position_embeddings = self.positional_embedding_table(positions) # (T, n_embed)\n",
    "\n",
    "        # we now represent X as a combination of token embeddings and positional embeddings.\n",
    "        # So, X contains not only the token identities, but the position at which they occur.\n",
    "        x = token_embeddings + position_embeddings # (B, T, n_embed)\n",
    "\n",
    "        # we pass the embeddings through the language model head\n",
    "        # to get the logits.\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"Predict the next token.\"\"\"\n",
    "        # get the logits\n",
    "        logits = self(x)\n",
    "        # get argmax\n",
    "        return logits.argmax(dim=-1)\n",
    "\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        \"\"\"Generate new tokens.\n",
    "        \n",
    "        idx is (B,T) shaped tensor with array of indices\n",
    "        that are in the current context.\n",
    "\n",
    "        At each step, we predict the next token and add it to the context.\n",
    "\n",
    "        Algorithm:\n",
    "            - We get the logits for the last token in the context for each sequence\n",
    "            in the batch\n",
    "            - We apply softmax to get probabilities for the next token, for each\n",
    "            sequence in the batch.\n",
    "            - We sample from the distribution to get the next token for each sequence\n",
    "            in the batch.\n",
    "            - We add the new token to the context and repeat.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get predictions\n",
    "            logits, _ = self(idx)\n",
    "            # focus on last time stamp\n",
    "            # becomes (B, C) since we take the logits of the\n",
    "            # last position in each sequence of the batch.\n",
    "            logits = logits[:, -1, :]\n",
    "            # apply softmax to get probabilities.\n",
    "            # (B, C), but now values are probabilities.\n",
    "            # Each row is a probability distribution whose values add\n",
    "            # up to 1.\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from the distribution\n",
    "            # (B, 1). We get the next token for each sample in the batch.\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            # we add the new token to the context and repeat.\n",
    "            # idx originally (B, T), next_token (B, 1), so when we concatenate\n",
    "            # we get (B, T+1)\n",
    "            idx = torch.cat([idx, next_token], dim=1) # (B, T+1)\n",
    "\n",
    "        # shape of idx is (B, T+max_new_tokens) since we've added\n",
    "        # max_new_tokens to the context.\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing self-attention\n",
    "\n",
    "Now we can get to what is the crux of self-attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch, time/context length/block size, embedding dimension / number of channels\n",
    "x = torch.randn(B, T, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement our same summation as before. Each of the 4 sampels in  the batch has shape `[8, 32]`. We multiply this by our `[8,8]` lower triangular softmax weight matrix. It takes a weighted running average of the entries up to entry $t$.\n",
    "\n",
    "For example, for the 5th entry in the list, we treat its representation as the weighted sum of all 5 `[1, 32]` embedding tensors up to position 5:\n",
    "$$\n",
    "    \\Sigma_{i=0}^{i=4}w_i\\text{embedding}_i = w_0e_0 + w_1e_1 + w_2e_2 + w_3e_3 + w_4e_4\\\\\n",
    "    w_i \\in \\R^1 \\\\\n",
    "    \\text{embedding}_i \\in \\R^{32}\n",
    "$$\n",
    "\n",
    "For each position, each embedding will be the token embedding for that token plus the positional embedding for that embedding. We then take a weighted sum of those.\n",
    "$$\\text{Embedding} = \\text{Token embedding} + \\text{Positional embedding}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wei.shape=torch.Size([8, 8])\n",
      "x.shape=torch.Size([4, 8, 32])\n",
      "wei=tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n",
      "out.shape=torch.Size([4, 8, 32])\n"
     ]
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float(\"-inf\"))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "print(f\"{wei.shape=}\")\n",
    "print(f\"{x.shape=}\")\n",
    "print(f\"{wei=}\")\n",
    "# print(f\"{x=}\")\n",
    "out = wei @ x\n",
    "print(f\"{out.shape=}\")\n",
    "# print(f\"{out=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the \"affinities\" or the weights of the weight matrix to a lower triangular matrix of zeros, since once we do the softmax of this, this will translate to an equally-weighted weight matrix of all tokens up to and including the tokens at position $t$.\n",
    "\n",
    "In practice, we don't actually want these to be all equally weighted. We want different weights, corresponding to how \"important\" a particular token is to the context of the token at position $t$. We want higher weights to corresopnd to higher affinities, or our model learning that that particular token is more important than other tokens to understanding the meaning of the token in question.\n",
    "\n",
    "Put differently, different tokens will find different other tokens interesting, and we want the weights to reflect that.\n",
    "\n",
    "Self-attention solves the problem of getting information from the past, but selectively choosing which information is deemed relevant and doing so in a data-dependent way.\n",
    "\n",
    "### How self-attention works\n",
    "Our goal is to get a weight matrix that will tell us how important each token is to understanding the token at position $t$. That is our end objective. We know that we'll have succeeded if we get an input sequence and for any arbitrary position $t$, we can take a weighted sum of tokens up to and including $t$ that will help us \"understand token $t$.\n",
    "\n",
    "One key innovation of self-attention is that it **decomposes the weight matrix into two matrices, the key and the query matrices**.\n",
    "\n",
    "Every single token at each position emits two vectors, a **key** and a **query** tensor. Roughly speaking,\n",
    "- **Query** vectors tell \"what am I looking for?\"\n",
    "- **Key** vectors tell \"this is what information I contain\"\n",
    "\n",
    "#### How queries and keys interact\n",
    "We get affinities between tokens in a sequence by taking to dot product of the query and key vectors. For a given token, we take its query vector and dot product against all the other key vectors of the sequence. The higher the dot product between token $a$'s query vector and token $b$'s key vector, the more that b \"answers\" a's query, meaning that b is more important to understanding the meaning of a, so a puts more weight to token b and \"attends\" to it more.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention heads\n",
    "We perform \"self-attention\" in an \"Attention Head\", so let's see it in action.\n",
    "\n",
    "Let's initialize key and query matrices and do the computation. Each key and query matrix will take in a token embedding and output a tensor of size \"head_size\".\n",
    "\n",
    "$$\\text{Embedding} \\rightarrow \\text{Key}, \\text{Query}$$\n",
    "\n",
    "For each token, we take its token embedding, pass it into the key and query matrices, and get a key and query representation:\n",
    "$$\\text{Token embedding} \\in \\R^{32} \\rightarrow key(\\text{embedding}), query(\\text{embedding}) \\rightarrow \\text{Key} \\in \\R^{16}, \\text{Query} \\in \\R^{16}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape=torch.Size([4, 8, 32])\n",
      "k.shape=torch.Size([4, 8, 16])\n",
      "q.shape=torch.Size([4, 8, 16])\n"
     ]
    }
   ],
   "source": [
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)\n",
    "q = query(x)\n",
    "print(f\"{x.shape=}\") # (B, T, 32)\n",
    "# for each 32-d embedding in the [4,8] batch sequences we get a 16-d key and query.\n",
    "print(f\"{k.shape=}\") # (B, T, 16)\n",
    "print(f\"{q.shape=}\") # (B, T, 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All 32 tokens in the $[4,8]$ matrix each get a key and query vector. These haven't interacted with each other yet, so now we need to do that interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to transpose the last two dimensions of k, since what we want is (T, 16) @ (16, T) -> (T, T) (and since it does it in parallel for each batch element, we get (B, T, T))\n",
    "# We now have the weight matrix, represented as the matrix product of the key and query.\n",
    "wei = q @ k.transpose(-2, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wei.shape=torch.Size([4, 8, 8])\n",
      "B=1 of weight matrix (product of query and key matrices) before softmax: \n",
      "tensor([[1.0000,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.5186, 0.4814,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.3447, 0.3377, 0.3176,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4020, 0.2005, 0.2034, 0.1940,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.1829, 0.2702, 0.1690, 0.2027, 0.1753,   -inf,   -inf,   -inf],\n",
      "        [0.2205, 0.1504, 0.1565, 0.1490, 0.1502, 0.1734,   -inf,   -inf],\n",
      "        [0.1615, 0.1488, 0.1367, 0.1314, 0.1589, 0.1285, 0.1342,   -inf],\n",
      "        [0.1277, 0.1173, 0.1294, 0.1517, 0.1211, 0.1160, 0.1197, 0.1171]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "B=1 of weight matrix after softmax: \n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5093, 0.4907, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3371, 0.3348, 0.3281, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2899, 0.2370, 0.2377, 0.2354, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1965, 0.2144, 0.1938, 0.2004, 0.1950, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1758, 0.1639, 0.1649, 0.1637, 0.1639, 0.1677, 0.0000, 0.0000],\n",
      "        [0.1455, 0.1437, 0.1420, 0.1412, 0.1452, 0.1408, 0.1416, 0.0000],\n",
      "        [0.1253, 0.1240, 0.1255, 0.1284, 0.1245, 0.1239, 0.1243, 0.1240]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "out.shape=torch.Size([4, 8, 32])\n"
     ]
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, float(\"-inf\"))\n",
    "print(f\"{wei.shape=}\")\n",
    "print(f\"B=1 of weight matrix (product of query and key matrices) before softmax: \\n{wei[0]}\")\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "print(f\"B=1 of weight matrix after softmax: \\n{wei[0]}\")\n",
    "out = wei @ x\n",
    "print(f\"{out.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now can represent the weighs as the matrix product of the key and query matrices, each of which is a fully-connected neural network whose parameters can be learned! Now the weight elements are no longer just a constant, where we take equally weighted amounts of each element, but rather a learned weighing of each element.\n",
    "\n",
    "Let's say, for example, that we're the 5th token. The 5th token knows what content it has and its position in the sequence. Based on that info (which gets represented as the embedding of that 5th token, the sum of the token and the positional embeddings), creates a query (\"hey, I'm looking for this kind of stuff\").\n",
    "- e.g., One of the weights in the query tensor might represent the query \"I'm a consonant in position 8, I'm looking for vowels up to position 4\"\n",
    "- All nodes/previous tokens emit keys to answer the query, and when the node can answer the query being asked (\"yes I am a vowel up to position 4\"), the dot product between its key and the query is high.\n",
    "- Let's say that we get a good answer on position = 3.\n",
    "- **The resulting weight matrix, $T * T$ represents queries (as rows) and keys (as columns).**\n",
    "- Therefore, we would get a dot product, $q_4 * k_2$, at position $\\text{wei}[4, 2]$, that is quite high, meaning that the token at position 3 is important to understanding the token at position 5, ro the token at position 5 \"attends\" to the token at position 3.\n",
    "- We do a row-wise softmax (i.e., we normalize across the dot products of a given query against all keys), which compresses the weights to have a sum of 1 and disproportionately increases the weighing of very strong dot products and decreases the weighing of small/negative dot products.\n",
    "\n",
    "The weight matrix is the matrix multiplication of the query and key tensor matrices. For self-attention, we need to mask out the upper right of the matrix, to avoid leaking future information. We then softmax across each row (corresponding to queries) such that the weighing is normalized across all the keys responding to a given query.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding the last component of the self-attention head: the value matrix\n",
    "\n",
    "We now have one more part that we have to include in order to complete our \"self-attention head\". We need to add a \"value\" tensor matrix.\n",
    "\n",
    "The value tensor holds the actual information that needs to be aggregated based on the attention scores. So, insead of $qk * x$, we actually want $qk * v$, where $v = Value(x)$.\n",
    "\n",
    "##### Why use a value matrix instead of the raw input embededings?\n",
    "Let's take an example. Let's say that we have the sentence \"The run outdoors was long and hot\", and let's say that we're looking at the word \"run\".\n",
    "\n",
    "Our embedding for the word \"run\" is going to be a combination of the token embedding (likely a word embedding coming from something like a BERT embedding or a Word2Vec model) and position embedding. The word embedding representation is likely an averaged out meaning of all the meanings that the word \"run\" can have (e.g., \"run\" as an action vs. \"run\" as a baseball term vs. \"run\" as a software development term).\n",
    "\n",
    "The key and query help us learn more about the  context of \"run\" in the sentence (e.g., \"how was the run?\", \"who was running?\", \"where was the run?\"). The value helps us learn what the word \"run\" actually means in the sentence (here, \"run\" as the act of movement as opposed to any other definition). We get a more context-specific representation of what the word \"run\" means.\n",
    "\n",
    "To take an overly simplified example, let's say that the word embedding for the word run has 3 components:\n",
    "$$\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "\\text{Run = act of moving quickly}\\\\\n",
    "\\text{Run = act of managing something}\\\\\n",
    "\\text{Run = a score in baseball}\\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "Let's say that our raw word embedding for the word \"run\" represents the word embedding for the word \"run\" as an equally weighted tensor of the three.\n",
    "$$\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "\\text{Run = act of moving quickly} = 0.33\\\\\n",
    "\\text{Run = act of managing something} = 0.33\\\\\n",
    "\\text{Run = a score in baseball} = 0.33\\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "\\rightarrow\n",
    "\\left[0.33, 0.33, 0.33\\right]\n",
    "$$\n",
    "\n",
    "Let's say that our value matrix is a randomly initialized tensor with head size of 4. For this case, then, it would have a shape of $[3, 4]$.  Let's say that in this random initialization, the resulting value tensor becomes $[0.25, 0.25, 0.25, 0.25]$ and that all 3 definitions of \"run\" from the word embedding are still weighed equally.\n",
    "\n",
    "Through backpropagation, what we want to see is that the \"value\" tensor that comes from passing the \"run\" embedding to the value matrix begins to more heavily weigh the \"Run = act of moving quickly\" parameter of the \"run\" embedding over the rest. The value matrix takes in the token and returns \"the meaning of this token in THIS context, as opposed to other contexts\". Put differently, the value tensor is a context-aware representation of the input (\"what does the word \"run\" mean in *this* context?\").\n",
    "\n",
    "##### How the query, key, and value interact\n",
    "The \"value\" tensor contains this representation of the word itself. It learns its own representation of what the word means. It's a richer representation than just the token or positional embeddings alone.\n",
    "\n",
    "The query and key matrices, when multiplied, create the weight matrix. This weight matrix, let's call it $W$, can then be multiplied by a value vector, $v$, to modify the representation of $v$ so that some parameters are weighed more and some are weighed less. This lets us change the representation of $v$ based on the context that we learned from the sequence.\n",
    "\n",
    "For example, let's say that we have a head size of 16, context length of 8, and embedding size of 32. Let's say that the 5th word is \"run\".\n",
    "\n",
    "The sequence that \"run\" is in would be a $[8,32]$ tensor, where the 5th element, at index $[4]$, would be $[1,32]$. We'd pass this to the value matrix, which changes the sequence from $[8,32] \\rightarrow [8,16]$, and the 5th element from $[1,32] \\rightarrow [1,16]$.\n",
    "\n",
    "The weight matrix would be $QK^{T} \\in \\R^{T*T}$. \n",
    "\n",
    "The final output is given by\n",
    "$$\n",
    "    A = \\text{Softmax}(\\frac{QK^{T}}{\\text{Normalization constant}}) \\\\ \n",
    "$$\n",
    "\n",
    "$$\n",
    "    A \\in \\R^{T*T}  \\\\\n",
    "    V \\in \\R^{T*16} = \\text{A 16-D value tensor for each of the $T$ tokens in the context length}\\\\\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\text{Output} = AV = \\R^{T*T} * \\R^{T*16} \\rightarrow \\R^{T*16} \\\\\n",
    "$$\n",
    "\n",
    "For the 5th element, we would take the tensor given by $A[4,:] \\in \\R^{T}$, which contains the attention weights for the 5th word \"run\" with respect to all other words in the sequence (the last 3 words are zeroed out, meaning we don't consider them when figuring out the meaning of \"run\"). These weights determine how much each word in the context contributes to the final representation of \"run\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another concrete example of how query, key, and value matrices interact\n",
    "For example, let's say we see the word \"treadmill\" as the second token in the sequence and we want to know the representation of \"run\" (e.g., \"I'll treadmill for my run today indoors\").\n",
    "\n",
    "The query tensor for \"run\" is $Q_4$ (for the 5th word). The key tensor for \"treadmill\" is $K_1$. The dot product should be high, let's say, $Q_4K_1^{T} = 0.8$ Therefore, the weight matrix $A$ would have the entry $A_{4,1} = 0.8$.\n",
    "\n",
    "**Attention weights**\n",
    "\n",
    "Let's say that after the softmax, \"treadmill\" is deemed to the context of \"run\" as very important compared to the other words in the sequence, so that $A_{4,1} = 0.9$.\n",
    "\n",
    "Let's say that $A[4,:] = [0.05, 0.9, 0.01, 0.01, 0.03, 0, 0, 0]$.\n",
    "\n",
    "**Value vectors**\n",
    "\n",
    "Let's now have some values for our value tensors:\n",
    "\n",
    "$$\n",
    "V =\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "V0\\\\\n",
    "V1\\\\\n",
    "V2\\\\\n",
    "V3\\\\\n",
    "V4\\\\\n",
    "V5\\\\\n",
    "V6\\\\\n",
    "V7\\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "V = \n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "[0.1, 0.2, 0.1, 0.2]\\\\\n",
    "[0.3, 0.1, 0.4, 0.2]\\\\\n",
    "[0.6, 0.2, 0.1, 0.1]\\\\\n",
    "[0.4, 0.7, 0.3, 0.3]\\\\\n",
    "[0.5, 0.2, 0.1, 0.1]\\\\\n",
    "[0.1, 0.3, 0.5, 0.5]\\\\\n",
    "[0.1, 0.1, 0.7, 0.3]\\\\\n",
    "[0.2, 0.5, 0.2, 0.1]\\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "\n",
    "**Output**\n",
    "\n",
    "Let's now calculate our output for position 5:\n",
    "$$\\text{Output}_4 = \\Sigma_{j=0}^{j=7}A_{4,j} * V_j$$\n",
    "\n",
    "Recall that $A[4,:] = [0.05, 0.9, 0.01, 0.01, 0.03, 0, 0, 0]$. If we do this calculation, then\n",
    "\n",
    "$$\\text{Output}_4 = (0.05 * [0.1, 0.2, 0.1, 0.2]) + (0.9 * [0.3, 0.1, 0.4, 0.2]) +\n",
    "    (0.01 * [0.6, 0.2, 0.1, 0.1]) + (0.01 * [0.4, 0.7, 0.3, 0.3]) + (0.03 * [0.5, 0.2, 0.1, 0.1])\n",
    "    + (0 * [0.1, 0.3, 0.5, 0.5]) + (0 * [0.1, 0.1, 0.7, 0.3]) + (0 * [0.2, 0.5, 0.2, 0.1])\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\text{Output}_4 = [0.3,0.115,0.372,0.197]\n",
    "$$\n",
    "\n",
    "**Interpretation**\n",
    "\n",
    "So, we represent the output for the 5th token, our learned meaning of the word \"run\" in this context, as a weighted sum of our learned representations of the other words in the sequence. We're saying that here, the meaning of the 5th token is most heavily driven by the word \"treadmill\".\n",
    "\n",
    "The query and key tell us which of the words (whose representations themselves are in the \"value\" tensors) are most important to understanding the meaning of the token that we care about.\n",
    "\n",
    "As we can see, our output vector for the word \"run\", $[0.3,0.115,0.372,0.197]$, is very similar to our value tensor for the word \"treadmill\" $[0.3, 0.1, 0.4, 0.2]$, signifying that the word \"treadmill\" has an outsized importance in determining the meaning of the word \"run\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together: the attention head\n",
    "\n",
    "The query, key, and value matrices, to put differently, do the following conceptually:\n",
    "- The query asks \"I need to know X,Y,Z to understand the meaning of the word I care about\"\n",
    "- The key says \"here is my answer to that query\". If the query is \"I need a word that is a noun\", then if the word is a noun, then $QK^T$ should be a higher number. The magnitude of the dot product tells you how well that key answers that query.\n",
    "- The attention calculation, the softmax of the normalized dot product between the queries and keys, tells you how much to \"pay attention\" to different words.\n",
    "- When the attention tensor is multiplied to the value tensors, we get a representation of what a certain token means as an aggregated sum of the words that came before it. For the previous example, we saw that the meaning of the word \"run\" in the sentence \"I'll treadmill for my run indoors today\", is 90% explained by the word \"treadmill\", hence the output for the word \"run\" is heavily weighed by the value tensor of the word \"treadmill\".\n",
    "\n",
    "This, in sum, is the attention head."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The attention head in action\n",
    "\n",
    "Let's see what this calculation looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape=torch.Size([4, 8, 32])\n",
      "k.shape=torch.Size([4, 8, 16])\n",
      "q.shape=torch.Size([4, 8, 16])\n",
      "v.shape=torch.Size([4, 8, 16])\n",
      "wei.shape=torch.Size([4, 8, 8])\n",
      "wei.shape=torch.Size([4, 8, 8])\n",
      "B=1 of weight matrix (product of query and key matrices) before softmax: \n",
      "tensor([[-1.7629,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-3.3334, -1.6556,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-1.0226, -1.2606,  0.0762,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [ 0.7836, -0.8014, -0.3368, -0.8496,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-1.2566,  0.0187, -0.7880, -1.3204,  2.0363,    -inf,    -inf,    -inf],\n",
      "        [-0.3126,  2.4152, -0.1106, -0.9931,  3.3449, -2.5229,    -inf,    -inf],\n",
      "        [ 1.0876,  1.9652, -0.2621, -0.3158,  0.6091,  1.2616, -0.5484,    -inf],\n",
      "        [-1.8044, -0.4126, -0.8306,  0.5898, -0.7987, -0.5856,  0.6433,  0.6303]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "B=1 of weight matrix after softmax: \n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
      "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
      "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "out.shape=torch.Size([4, 8, 16])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "print(f\"{x.shape=}\")\n",
    "k = key(x)\n",
    "q = query(x)\n",
    "v = value(x)\n",
    "print(f\"{k.shape=}\")\n",
    "print(f\"{q.shape=}\")\n",
    "print(f\"{v.shape=}\")\n",
    "\n",
    "wei = q @ k.transpose(-2, -1)\n",
    "print(f\"{wei.shape=}\")\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, float(\"-inf\"))\n",
    "print(f\"{wei.shape=}\")\n",
    "print(f\"B=1 of weight matrix (product of query and key matrices) before softmax: \\n{wei[0]}\")\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "print(f\"B=1 of weight matrix after softmax: \\n{wei[0]}\")\n",
    "\n",
    "# now let's actually get the weighted sum of the values.\n",
    "out = wei @ v\n",
    "print(f\"{out.shape=}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now encapsulate this in a programmatic form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    \"\"\"One head of self-attention.\"\"\"\n",
    "\n",
    "    def __init__(self, head_size, n_embed, context_length):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        self.n_embed = n_embed\n",
    "        self.context_length = context_length\n",
    "        self.dropout_p = 0.1\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(context_length, context_length)))\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x) # (B, T, head_size)\n",
    "        q = self.query(x) # (B, T, head_size)\n",
    "        v = self.value(x) # (B, T, head_size)\n",
    "\n",
    "        # compute attention scores\n",
    "        wei = q @ k.transpose(-2, -1)\n",
    "        wei = wei.masked_fill(self.tril == 0, float(\"-inf\"))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "        # perform the weighted sum of the values\n",
    "        out = wei @ v\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combine the results of the single heads into multiple heads that run in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multiple heads of self-attention in parallel.\"\"\"\n",
    "    def __init__(self, n_heads, head_size, n_embed, context_length):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.head_size = head_size\n",
    "        self.n_embed = n_embed\n",
    "        self.context_length = context_length\n",
    "        self.attention_heads = nn.ModuleList([\n",
    "            AttentionHead(head_size, n_embed, context_length) for _ in range(n_heads)\n",
    "        ])\n",
    "        self.linear = nn.Linear(n_heads * head_size, n_embed)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # for each head, we get the output of the attention head.\n",
    "        # we concatenate the outputs of the attention heads along the last dimension.\n",
    "        # we then pass the concatenated output through a linear layer to get the final output.\n",
    "        out = torch.cat([head(x) for head in self.attention_heads], dim=-1)\n",
    "        out = self.linear(out)\n",
    "        out = self.dropout(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next need a feedforward set of linear layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN(nn.Module):\n",
    "    \"\"\"Feed-forward neural network.\"\"\"\n",
    "    def __init__(self, n_embed, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(n_embed, hidden_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_dim, n_embed)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then combine this all into one Transformer block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Transformer block.\"\"\"\n",
    "\n",
    "    def __init__(self, n_heads, n_embed, context_length, hidden_dim):\n",
    "        super().__init__()\n",
    "        # since we concatenate the results of the different parallel heads together, we want\n",
    "        # the total concatenated output to equal n_embed.\n",
    "        head_size = n_embed // n_heads   \n",
    "        self.attention = MultiHeadAttention(n_heads, head_size, n_embed, context_length)\n",
    "        self.norm1 = nn.LayerNorm(n_embed)\n",
    "        self.ffnn = FFNN(n_embed, hidden_dim)\n",
    "        self.norm2 = nn.LayerNorm(n_embed)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # we first pass the input through the multi-head attention layer.\n",
    "        # we then add the input to the output of the multi-head attention layer.\n",
    "        # we then pass the output through a feed-forward neural network.\n",
    "        # we then add the output of the feed-forward neural network to the output of the multi-head attention layer.\n",
    "        # we then normalize the output.\n",
    "        x = x + self.attention(self.norm1(x))\n",
    "        x = x + self.ffnn(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then combine these blocks to create an end-to-end implementation of a GPT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size=65, block_size=8, n_embed=32, n_heads=2, hidden_dim=64, n_layers=2):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.positional_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(n_heads, n_embed, block_size, hidden_dim) for _ in range(n_layers)\n",
    "        ])\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "        self.ln = nn.LayerNorm(n_embed) # final layer norm.\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Add good weight initializations.\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        token_embeddings = self.token_embedding_table(idx) # (B, T, n_embed)\n",
    "        position_embeddings = self.positional_embedding_table(\n",
    "            torch.arange(T, device=idx.device)\n",
    "        ) # (T, n_embed)\n",
    "        x = token_embeddings + position_embeddings # (B, T, n_embed)\n",
    "        x = self.blocks(x) # (B, T, n_embed)\n",
    "        x = self.ln(x) # (B, T, n_embed)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] # (B, C)\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C) of probs\n",
    "            # sample from distribution\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            # add new token to context\n",
    "            idx = torch.cat([idx, next_token], dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural-networks-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
